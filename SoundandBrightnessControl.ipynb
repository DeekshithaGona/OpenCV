{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1ba0f6",
   "metadata": {},
   "source": [
    "# 1.sound function 2.brightness function 3.peace sign detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d00afc",
   "metadata": {},
   "source": [
    "\n",
    "**Project Title**: Hand Gesture Control for Audio Volume and Screen Brightness\n",
    "\n",
    "**Project Description**:\n",
    "The Hand Gesture Control project aims to provide an interactive way of adjusting system settings, specifically audio volume and screen brightness, using hand gestures captured by a webcam. The project utilizes the Mediapipe library for hand tracking, OpenCV for image processing, and external libraries for controlling audio volume and screen brightness.\n",
    "\n",
    "**Key Features**:\n",
    "1. **Hand Gesture Recognition**: The project employs the Mediapipe library to accurately detect and track hand landmarks in real-time using a webcam feed. It identifies specific gestures by analyzing the spatial arrangement of hand landmarks.\n",
    "\n",
    "2. **Audio Volume Control**: By recognizing specific hand gestures, the project allows users to control the system's audio volume. Gestures, such as \"volume up\" and \"volume down,\" are detected and translated into appropriate actions using the pycaw library to interact with the system's audio settings.\n",
    "\n",
    "3. **Screen Brightness Control**: Similarly, users can adjust the screen brightness through hand gestures. Recognizing gestures like \"increase brightness\" and \"decrease brightness,\" the project utilizes the screen_brightness_control library to interact with the system's screen brightness settings.\n",
    "\n",
    "4. **Toggle Functionality**: A peace sign gesture is used to switch between audio volume and screen brightness control modes. When the peace sign is detected, the system switches between the two modes, allowing users to seamlessly alternate between controlling audio volume and screen brightness.\n",
    "\n",
    "**Implementation**:\n",
    "The project starts by initializing the webcam feed and configuring the hand tracking models provided by the Mediapipe library. The script processes the webcam frames, detects hand gestures using landmark coordinates, and visualizes the tracked landmarks and gestures using OpenCV's drawing functions.\n",
    "\n",
    "The interaction with the system's audio volume and screen brightness settings is achieved using external libraries (pycaw and screen_brightness_control) that provide interfaces to interact with Windows APIs.\n",
    "\n",
    "**User Interaction**:\n",
    "1. Users can tap their thumb and index finger or thumb and middle finger to control the audio volume.\n",
    "2. Tapping thumb and index , thumb and middle finger controls the screen brightness.\n",
    "3. Displayed visuals indicate the current audio volume or screen brightness level.\n",
    "4. A peace sign gesture toggles between audio and brightness control modes.\n",
    "\n",
    "**Usage Scenarios**:\n",
    "- Adjusting audio volume or screen brightness during presentations or media playback without directly interacting with the system.\n",
    "- Creating a unique and engaging way to control system settings for interactive installations or demonstrations.\n",
    "- Demonstrating the potential of computer vision and gesture recognition in user interfaces.\n",
    "\n",
    "**Future Enhancements**:\n",
    "- Incorporating more gestures for additional functionalities, like skipping tracks or locking the screen.\n",
    "- Expanding gesture recognition to include more complex hand movements for more precise control.\n",
    "- Making the application cross-platform by integrating with other operating systems.\n",
    "\n",
    "**Note**: The project's functionality and implementation assume the use of Windows OS due to the specific libraries used for audio and screen brightness control. If the project is intended for other operating systems, adjustments would be needed to work with the corresponding APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1daa45",
   "metadata": {},
   "source": [
    "# final code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d41630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brightness level: 0\n",
      "brightness level: 0\n",
      "brightness level: 0\n",
      "brightness level: 0\n",
      "brightness level: 0\n",
      "brightness level: 2\n",
      "brightness level: 2\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 4\n",
      "brightness level: 6\n",
      "brightness level: 6\n",
      "brightness level: 6\n",
      "brightness level: 8\n",
      "brightness level: 10\n",
      "brightness level: 10\n",
      "brightness level: 12\n",
      "brightness level: 14\n",
      "brightness level: 14\n",
      "brightness level: 14\n",
      "brightness level: 16\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 18\n",
      "brightness level: 20\n",
      "brightness level: 22\n",
      "brightness level: 22\n",
      "brightness level: 22\n",
      "brightness level: 22\n",
      "brightness level: 24\n",
      "brightness level: 24\n",
      "brightness level: 24\n",
      "brightness level: 24\n",
      "brightness level: 26\n",
      "brightness level: 26\n",
      "brightness level: 26\n",
      "brightness level: 28\n",
      "brightness level: 28\n",
      "brightness level: 30\n",
      "brightness level: 30\n",
      "brightness level: 30\n",
      "brightness level: 30\n",
      "brightness level: 32\n",
      "brightness level: 34\n",
      "brightness level: 36\n",
      "brightness level: 36\n",
      "brightness level: 38\n",
      "brightness level: 38\n",
      "brightness level: 40\n",
      "brightness level: 42\n",
      "brightness level: 42\n",
      "brightness level: 42\n",
      "brightness level: 42\n",
      "brightness level: 42\n",
      "brightness level: 44\n",
      "brightness level: 44\n",
      "brightness level: 44\n",
      "brightness level: 44\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "import screen_brightness_control as sbc\n",
    "import pyautogui\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from time import sleep\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Function to control sound\n",
    "def control_sound(img,res):\n",
    "    #imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #r = hands.process(img)\n",
    "    r = res\n",
    "    if r.multi_hand_landmarks:\n",
    "        for handLMS in r.multi_hand_landmarks:\n",
    "            lmList = []\n",
    "            for idaa, lm in enumerate(handLMS.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "                lmList.append([idaa, cx, cy])\n",
    "                mpDraws.draw_landmarks(img, handLMS, mediahands.HAND_CONNECTIONS)\n",
    "            \n",
    "            if lmList:\n",
    "                x1, y1 = lmList[4][1], lmList[4][2]\n",
    "                x2, y2 = lmList[8][1], lmList[8][2]\n",
    "                ox, oy = lmList[12][1], lmList[12][2]\n",
    "                \n",
    "                cv2.circle(img, (x1, y1), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.circle(img, (x2, y2), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.circle(img, (ox, oy), 10, (255, 0, 255), cv2.FILLED)\n",
    "                \n",
    "                length = math.hypot(x2 - x1, y2 - y1)\n",
    "                down = math.hypot(x1 - ox, y1 - oy)\n",
    "                \n",
    "                ca = volume.GetMasterVolumeLevelScalar() * 100\n",
    "                #print(int(ca))\n",
    "                \n",
    "                vbar = np.interp(ca, [0, 100], [400, 150])\n",
    "                cv2.rectangle(img, (50, 150), (85, 400), (0, 255, 0), 3)\n",
    "                cv2.rectangle(img, (50, int(vbar)), (85, 400), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(img, f'{int(ca)}%', (40, 450), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 255), 3)\n",
    "\n",
    "                if down >= 15 and down <= 20:\n",
    "                    pyautogui.press('volumedown')\n",
    "                    \n",
    "                if length >= 15 and length <= 20:\n",
    "                    pyautogui.press('volumeup')\n",
    "                text = 'OP Sound'\n",
    "                if int(ca) == 100 or int(ca) > 80:\n",
    "                    text = 'High volume, unsafe to continue at this rate. Do you still want to continue'\n",
    "                    #print('High volume, unsafe to continue at this rate. Do you still want to continue?')\n",
    "                    \n",
    "                \n",
    "                if int(ca) == 0:\n",
    "                    text = 'Mute'\n",
    "                    #print('Mute')\n",
    "                cv2.putText(img,text,(10,50),cv2.FONT_HERSHEY_PLAIN,2,(0,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "# Function to control brightness\n",
    "def control_brightness(img,res):\n",
    "    #imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #results = hands.process(img)\n",
    "    r = res\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            lmList = []\n",
    "            for idaa, lm in enumerate(handLms.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([idaa, cx, cy])\n",
    "                mpDraws.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "            if lmList:\n",
    "                x1, y1 = lmList[4][1], lmList[4][2]\n",
    "                x2, y2 = lmList[8][1], lmList[8][2]\n",
    "                ox, oy = lmList[12][1], lmList[12][2]\n",
    "                cv2.circle(img, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
    "\n",
    "                length = math.hypot(x2 - x1, y2 - y1)\n",
    "                down = math.hypot(x1 - ox, y1 - oy)\n",
    "                kk = sbc.get_brightness()[0]\n",
    "                print(\"brightness level:\", sbc.get_brightness()[0])\n",
    "                \n",
    "                vbar = np.interp(kk, [0, 100], [400, 150])\n",
    "                cv2.rectangle(img, (50, 150), (85, 400), (0, 255, 0), 3)\n",
    "                cv2.rectangle(img, (50, int(vbar)), (85, 400), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(img, f'{int(kk)}%', (40, 450), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 255), 3)\n",
    "                \n",
    "                if down >= 15 and down <= 20:\n",
    "                    sbc.set_brightness(sbc.get_brightness()[0] - 2)\n",
    "                if length >= 15 and length <= 20:\n",
    "                    sbc.set_brightness(sbc.get_brightness()[0] + 2)\n",
    "                text = 'Op brightness'\n",
    "                cv2.putText(img,text,(10,50),cv2.FONT_HERSHEY_PLAIN,2,(0,255,255),2,cv2.LINE_AA)\n",
    "# Function to check if the peace sign gesture is detected\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    if (\n",
    "        len(hand_landmarks.landmark) >= 4 and\n",
    "        hand_landmarks.landmark[2].y < hand_landmarks.landmark[3].y and\n",
    "        hand_landmarks.landmark[2].y < hand_landmarks.landmark[4].y\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Initialize Mediapipe hand detection\n",
    "mediahands = mp.solutions.hands \n",
    "hands = mediahands.Hands()\n",
    "mpDraws = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize drawing utilities\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get audio devices and initialize volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL,None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "# Toggle flag between sound control and brightness control\n",
    "toggle_flag = True\n",
    "\n",
    "while True:\n",
    "    s, img = cap.read()\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(imgRGB)\n",
    "    \n",
    "    if toggle_flag:\n",
    "        # Control sound\n",
    "        control_sound(img,results)\n",
    "    else:\n",
    "        # Control brightness\n",
    "        control_brightness(img,results)\n",
    "    \n",
    "    # Detect peace sign gesture\n",
    "   \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mpDraws.draw_landmarks(img, hand_landmarks, mpHands.HAND_CONNECTIONS)\n",
    "            \n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                text = 'changing'\n",
    "                cv2.putText(img,text,(10,90),cv2.FONT_HERSHEY_PLAIN,2,(0,255,255),2,cv2.LINE_AA)\n",
    "                toggle_flag = not toggle_flag\n",
    "                sleep(0.5)\n",
    "                \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad821935",
   "metadata": {},
   "source": [
    "In summary, the future scope of the project involves gesture customization, integration with various applications, advanced computer vision techniques, gesture feedback and guidance, real-world applications, security and privacy considerations, and gesture learning and adaptation. These enhancements would further elevate the system's usability, flexibility, and potential for diverse applications in the field of human-computer interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180254e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
